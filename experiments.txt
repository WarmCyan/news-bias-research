FROM THIS POINT NO BIAS EXPERIMENT IS USING SPIEGEL
NOTE: for all of below (if on cluster) run first one first
+ means no dependency on former?

order that I've done these:
1. rel_avg
2. bias_avg
3. bias_sel
4. seq_bias_embed
5. bias_embed
6. seq_bias_sel
7. rel_sentic
8. bias_sentic
9. bias_avg_arch
10. seq_rel_embed
11. bias_seq_arch
12. rel_sel
13. rel_embed


======================================
To establish avg_std better than avg
"bias_avg"

bias_svm_w2v_avg (160)
bias_svm_w2v_avg_std (370)
bias_w2v_avg (0)
bias_w2v_avg_std (10)

compile_results.py -e run_0/bias_svm_w2v_avg,run_0/bias_svm_w2v_avg_std,run_0/bias_w2v_avg,run_0/bias_w2v_avg_std -o bias_avg_0

compile_results.py -e run_1/bias_svm_w2v_avg,run_1/bias_svm_w2v_avg_std,run_1/bias_w2v_avg,run_1/bias_w2v_avg_std -o bias_avg_1


compile_results.py -e run_0/bias_svm_w2v_avg,run_0/bias_svm_w2v_avg_std,run_0/bias_w2v_avg,run_0/bias_w2v_avg_std,run_1/bias_svm_w2v_avg,run_1/bias_svm_w2v_avg_std,run_1/bias_w2v_avg,run_1/bias_w2v_avg_std -o bias_avg_combined

Times run: 2

Conclusions:
* SVM seems to do better than NN
* std_dev did slightly better for NN, not for SVM

======================================
To establish better selection set:
"bias_sel"

bias_svm_w2v_avg (160)
bias_svm_w2v_avg_allsides_all (380)
bias_svm_w2v_avg_mbm (390)
bias_svm_w2v_avg_below20flip (400)
bias_svm_w2v_avg_below25flip (410)

compile_results.py -e run_0/bias_svm_w2v_avg,run_0/bias_svm_w2v_avg_allsides_all,run_0/bias_svm_w2v_avg_mbm,run_0/bias_svm_w2v_avg_below20flip,run_0/bias_svm_w2v_avg_below25flip -o bias_sel_0

Times run: 1

Conclusions:
* Collective labelling did better than allsides, slightly worse than MBM (on AL, collective actually did better than both
* Flipped labels on collective did do better than mbm

======================================
To establish better selection set:
"rel_sel"

rel_svm_w2v_avg (350)
rel_svm_w2v_avg_ng (700)
rel_svm_w2v_avg_mbfc (710)


compile_results.py -e run_0/rel_svm_w2v_avg,run_0/rel_svm_w2v_avg_ng,run_0/rel_svm_w2v_avg_mbfc -o rel_sel_0

Times run: 1

Conclusions:
* Compiled is better on source, _barely_ better than MBFC on AL, and slightly worse on AL unseen, definitely **run this again**


======================================
To establish better selection set with sequence data:
"seq_bias_sel"

bias_w2v_seq (60)
bias_w2v_seq_allsides_all (440)
bias_w2v_seq_mbm (450)
bias_w2v_seq_below20flip (460)
bias_w2v_seq_below25flip (470)

Times run: 1

compile_results.py -e seq/bias_w2v_seq,seq/bias_w2v_seq_allsides_all,seq/bias_w2v_seq_mbm,seq/bias_w2v_seq_below20flip,seq/bias_w2v_seq_below25flip -o selection_seqs

Conclusions:
* similar to bias_sel, collective better on source than all others, MBM better on AL, but the flip25 did best everywhere

======================================
To establish superior embedding:
"bias_embed"

bias_svm_tfidf (140)
bias_svm_w2v_avg (350)
bias_svm_glove_avg (420)
bias_svm_ft_avg (430)
# bias_tfidf
# bias_w2v_avg
# bias_glove_avg
# bias_ft_avg

compile_results.py -e run_0/bias_svm_tfidf,run_0/bias_svm_w2v_avg,run_0/bias_svm_glove_avg,run_0/bias_svm_ft_avg -o bias_embed_0

Times run: 1

Conclusions:
* W2V worse on source-level tests, but much better generalization to AL
* TFIDF worst on source-level

======================================
To establish superior embedding for sequences:
"seq_bias_embed"

bias_w2v_seq (60)
bias_glove_seq (70)
bias_ft_seq (80)

compile_results.py -e seq/bias_w2v_seq,seq/bias_glove_seq,seq/bias_ft_seq -o seq_bias_embed

Times run: 1

Conclusions:
* W2V better in all instances

======================================
To establish superior embedding for sequences with reliability:
"seq_rel_embed"

rel_w2v_seq (670)
rel_glove_seq (680)
rel_ft_seq (690)

compile_results.py -e seq/rel_w2v_seq,seq/rel_glove_seq,seq/rel_ft_seq -o seq_rel_embed

Times run: 1

Conclusions:
* Glove did better on source-level, and did marginally better on AL, but only by a small amount (they were all very close)

======================================
To establish avg_std better than avg
"rel_avg"

rel_svm_w2v_avg (350)
rel_svm_w2v_avg_std (360)
rel_w2v_avg (300)
rel_w2v_avg_std (310)

compile_results.py -e run_0/rel_svm_w2v_avg,run_0/rel_svm_w2v_avg_std,run_0/rel_w2v_avg,run_0/rel_w2v_avg_std -o rel_avg_0

compile_results.py -e run_1/rel_svm_w2v_avg,run_1/rel_svm_w2v_avg_std,run_1/rel_w2v_avg,run_1/rel_w2v_avg_std -o rel_avg_1

Times run: 2

Conclusions:
* avg appears to outperform avg_std by a small amount in all AL cases
* SVM outperforms NN

======================================
To establish sentics with reliability:
"rel_sentic"

rel_svm_w2v_avg (350) [run]
rel_svm_w2v_limit_avg (480)
rel_svm_w2v_sentic_avg (490)
rel_svm_w2v_sentic_full_avg (500)
rel_svm_w2v_sentic_avg_std (510)
rel_svm_w2v_sentic_full_avg_std (520)

compile_results.py -e run_0/rel_svm_w2v_avg,run_0/rel_svm_w2v_limit_avg,run_0/rel_svm_w2v_sentic_avg,run_0/rel_svm_w2v_sentic_full_avg,run_0/rel_svm_w2v_sentic_avg_std,run_0/rel_svm_w2v_sentic_full_avg_std, -o rel_sentic_0

Times run: 1

Conclusions:
* sentic_full_avg did a _tiny bit_ better than avg on AL, all others (including with avg_std) did worse than regular


======================================
To establish sentics with bias:
"bias_sentic"

bias_svm_w2v_avg (370) [run]
bias_svm_w2v_limit_avg (530)
bias_svm_w2v_sentic_avg (540)
bias_svm_w2v_sentic_full_avg (550)
bias_svm_w2v_sentic_avg_std (560)
bias_svm_w2v_sentic_full_avg_std (570)

compile_results.py -e run_0/bias_svm_w2v_avg,run_0/bias_svm_w2v_limit_avg,run_0/bias_svm_w2v_sentic_avg,run_0/bias_svm_w2v_sentic_full_avg,run_0/bias_svm_w2v_sentic_avg_std,run_0/bias_svm_w2v_sentic_full_avg_std -o bias_sentic_0

Times run: 1

Conclusions:
* Plain does better than all sentic runs for AL





======================================
To establish seq versus avg

TODO (which embedding? [w2v])



======================================
Architecture search for avg bias
"bias_avg_arch"
NOTE: these are all run on below25flip selection set

bias_w2v_avg_arch1 (64, 1) [1] (580)
bias_w2v_avg_arch2 (128, 1) [1] (590)
bias_w2v_avg_arch3 (256, 1) [1] (600)
bias_w2v_avg_arch4 (128, 64, 1) [2] (610)
bias_w2v_avg_arch5 (256, 128, 1) [2] (620)
bias_w2v_avg_arch6 (256, 256, 1) [2] (630)
bias_w2v_avg_arch7 (256, 128, 64, 1) [3] (640)
bias_w2v_avg_arch8 (256, 256, 256, 1) [3] (650)

compile_results.py -e run_0/bias_w2v_avg_arch1,run_0/bias_w2v_avg_arch2,run_0/bias_w2v_avg_arch3,run_0/bias_w2v_avg_arch4,run_0/bias_w2v_avg_arch5,run_0/bias_w2v_avg_arch6,run_0/bias_w2v_avg_arch7,run_0/bias_w2v_avg_arch8 -o bias_avg_arch_0

Times run: 1

Conclusions:
* Surprisingly little variance, everything within 2%
* Arch 4 generally did very slightly better than arch 2 [the latter of which is what I've been using for most]
* Top 3 in order: 4, 2, 5, 

======================================
Architecture search for seq bias
"bias_seq_arch"
NOTE: these are all run on below25flip selection set

bias_w2v_seq_arch1 (128, 1) [1] (720)
bias_w2v_seq_arch2 (256, 1) [1] (730)
bias_w2v_seq_arch3 (128, 128, 1) [2] (740)
bias_w2v_seq_arch4 (128, 128, 128, 1) [4] (750)
bias_w2v_seq_arch5 (256, 128, 64, 1) [4] (760)


compile_results.py -e seq/bias_w2v_seq_arch1,seq/bias_w2v_seq_arch2,seq/bias_w2v_seq_arch3,seq/bias_w2v_seq_arch4,seq/bias_w2v_seq_arch5 -o bias_seq_arch

Times run: 1

Conclusions: 
* While 2 performs the best on source-level, arch 1 (the one I've been using) generalizes the best

-------------------------------
Standalone experiments:

does sentic work better on below25flip and with larger arch?
bias_w2v_sentic_full_avg_std_arch4 (660)


compile_results.py -e run_0/bias_w2v_avg_arch4,run_0/bias_w2v_sentic_full_avg_std_arch4,run_0/bias_svm_w2v_avg,run_0/bias_svm_w2v_sentic_full_avg_std -o senticandflipandarch

Times run: 1

Conclusions:
* Nope, sentics still worse


======================================
Determining which embedding is better for reliability
"rel_embed"

rel_svm_w2v_avg (770)
rel_svm_glove_avg (780)
rel_svm_ft_avg (790)
rel_svm_tfidf (800)

compile_results.py -e run_0/rel_svm_w2v_avg,run_0/rel_svm_glove_avg,run_0/rel_svm_ft_avg,run_0/rel_svm_tfidf -o rel_embed_0 


Times run: 1

Conclusions:


======================================
Determining which embedding is better for reliability
"dir_embed"

dir_svm_w2v_avg (810)
dir_svm_glove_avg (820)
dir_svm_ft_avg (830)
dir_svm_tfidf (840)

compile_results.py -e run_0/dir_svm_w2v_avg,run_0/dir_svm_glove_avg,run_0/dir_svm_ft_avg,run_0/dir_svm_tfidf -o dir_embed_0

Times run: 1

Conclusions:


======================================
Determining which embedding is better for reliability
"dir_sel"

NO:
dir_svm_w2v_avg (810) [run]
dir_svm_w2v_avg_allsides_all (850)
dir_svm_w2v_avg_mbm (860)
dir_svm_w2v_avg_dir_below20flip (870) 

YES:
dir_svm_glove_avg (820) [run]
dir_svm_glove_avg_allsides_all (970)
dir_svm_glove_avg_mbm (980)
dir_svm_glove_avg_dir_below20flip (990)


======================================
Determining which embedding is better for reliability
"dir_sentic"

NO MORE
dir_svm_w2v_avg (810) [run]
dir_svm_w2v_limit_avg (890)
dir_svm_w2v_sentic_avg (900)
dir_svm_w2v_sentic_full_avg (910)
dir_svm_w2v_sentic_avg_std (920)
dir_svm_w2v_sentic_full_avg_std (930)


dir_svm_glove_avg (1000)
dir_svm_glove_limit_avg (1010)
dir_svm_glove_sentic_avg (1020)
dir_svm_glove_sentic_full_avg (1030)


compile_results.py -e run_0/dir_svm_w2v_avg,run_0/dir_svm_w2v_limit_avg,run_0/dir_svm_w2v_sentic_avg,run_0/dir_svm_w2v_sentic_full_avg,run_0/dir_svm_w2v_sentic_avg_std,run_0/dir_svm_w2v_sentic_full_avg_std -o dir_sentic_0


RERUN WITH GLOVE


Times run: 1

Conclusions:

======================================
Determining which embedding is better for direction of bias for sequences
"seq_dir_embed"

dir_w2v_seq (940)
dir_glove_seq (950)
dir_ft_seq (960)


compile_results.py -e seq/dir_w2v_seq,seq/dir_glove_seq,seq/dir_ft_seq -o seq_dir_embed_0









Analysis runs:


Supporting
##########

NN-arch search
--------------

compile_results.py \
-e run_0/bias_w2v_avg_arch1,run_0/bias_w2v_avg_arch2,run_0/bias_w2v_avg_arch3,run_0/bias_w2v_avg_arch4,run_0/bias_w2v_avg_arch5,run_0/bias_w2v_avg_arch6,run_0/bias_w2v_avg_arch7,run_0/bias_w2v_avg_arch8 \
-o bias_avg_arch \
--caption "Neural network achitecture search" \
--column-replacements "bias_w2v_avg_arch1=Arch 1,bias_w2v_avg_arch2=Arch 2,bias_w2v_avg_arch3=Arch 3,bias_w2v_avg_arch4=Arch 4,bias_w2v_avg_arch5=Arch 5,bias_w2v_avg_arch6=Arch 6,bias_w2v_avg_arch7=Arch 7,bias_w2v_avg_arch8=Arch 8"


LSTM-arch search
----------------

compile_results.py \
-e seq/bias_w2v_seq_arch1,seq/bias_w2v_seq_arch2,seq/bias_w2v_seq_arch3,seq/bias_w2v_seq_arch4 \
-o bias_seq_arch \
--caption "LSTM architecture search" \
--column-replacements "bias_w2v_seq_arch1=Arch 1,bias_w2v_seq_arch2=Arch 2,bias_w2v_seq_arch3=Arch 3,bias_w2v_seq_arch4=Arch 4"


Average with standard deviation (reliability)
---------------------------------------------

compile_results.py \
-e run_0/rel_svm_w2v_avg,run_0/rel_svm_w2v_avg_std,run_0/rel_w2v_avg,run_0/rel_w2v_avg_std,run_1/rel_svm_w2v_avg,run_1/rel_svm_w2v_avg_std,run_1/rel_w2v_avg,run_1/rel_w2v_avg_std \
-o rel_avg \
--caption "Average vector versus average and standard deviation vector."
--column-replacements "rel_svm_w2v_avg=Average,rel_svm_w2v_avg_std=Average + \\sigma"


Reliability
###########

Embedding
---------

compile_results.py \
-e run_0/rel_svm_w2v_avg,run_0/rel_svm_glove_avg,run_0/rel_svm_ft_avg,run_0/rel_svm_tfidf \
-o rel_embed \
--caption "Embedding comparisons on predicting reliability." \
--column-replacements "rel_svm_w2v_avg=Word2Vec,rel_svm_glove_avg=GloVe,rel_svm_ft_avg=FastText,rel_svm_tfidf=TF-IDF"


Selection set
-------------

compile_results.py \
-e run_0/rel_svm_w2v_avg,run_0/rel_svm_w2v_avg_ng,run_0/rel_svm_w2v_avg_mbfc \
-o rel_sel \
--caption "Reliability selection set comparison." \
--column-replacements "rel_svm_w2v_avg=Combined,rel_svm_w2v_avg_mbfc=MB/FC,rel_svm_w2v_avg_ng=NewsGuard"


Sentics
-------

compile_results.py \
-e run_0/rel_svm_w2v_avg,run_0/rel_svm_w2v_limit_avg,run_0/rel_svm_w2v_sentic_avg,run_0/rel_svm_w2v_sentic_full_avg,run_0/rel_svm_w2v_sentic_avg_std,run_0/rel_svm_w2v_sentic_full_avg_std \
-o rel_sentic \
--caption "Sentics incorporation." \
--column-replacements "rel_svm_w2v_avg=No sentics,rel_svm_w2v_limit_avg=Limited,rel_svm_w2v_sentic_avg=Sentics,rel_svm_w2v_sentic_full_avg=Padded sentics,rel_svm_w2v_sentic_avg_std=Sentics (\\sigma),rel_svm_w2v_sentic_full_avg_std=Padded sentics (\\sigma)"

**NOTE: the alternative is to not include the standard deviation columns:**

compile_results.py \
-e run_0/rel_svm_w2v_avg,run_0/rel_svm_w2v_limit_avg,run_0/rel_svm_w2v_sentic_avg,run_0/rel_svm_w2v_sentic_full_avg,run_0/rel_svm_w2v_sentic_avg_std,run_0/rel_svm_w2v_sentic_full_avg_std \
-o rel_sentic \
--caption "Sentics incorporation." \
--column-replacements "rel_svm_w2v_avg=No sentics,rel_svm_w2v_limit_avg=Limited,rel_svm_w2v_sentic_avg=Sentics,rel_svm_w2v_sentic_full_avg=Padded sentics"


ML Algorithm
------------

compile_results.py \
-e seq/rel_w2v_seq,run_0/rel_svm_w2v_avg,run_0/rel_w2v_avg \
-o rel_alg \
--caption "ML algorithm comparison." \
--column-replacements "rel_svm_w2v_avg=SVM,rel_w2v_avg=NN,rel_w2v_seq=LSTM"





Bias
####


Embedding
---------

compile_results.py \
-e run_0/bias_svm_tfidf,run_0/bias_svm_w2v_avg,run_0/bias_svm_glove_avg,run_0/bias_svm_ft_avg \
-o bias_embed \
--caption "Embedding comparison in biased versus unbiased predictions." \
--column-replacements "bias_svm_w2v_avg=Word2Vec,bias_svm_glove_avg=GloVe,bias_svm_ft_avg=FastText,bias_svm_tfidf=TF-IDF"


Selection set
-------------

compile_results.py \
-e run_0/bias_svm_w2v_avg,run_0/bias_svm_w2v_avg_allsides_all,run_0/bias_svm_w2v_avg_mbm,run_0/bias_svm_w2v_avg_below20flip,run_0/bias_svm_w2v_avg_below25flip \
-o bias_sel \
--caption "Bias labeling selection set comparison." \
--column-replacements "bias_svm_w2v_avg=Combined,bias_svm_w2v_avg_allsides_all=AllSides,bias_svm_w2v_avg_mbm=MBM,bias_svm_w2v_avg_below20flip=Combined (\\textless20\\%),bias_svm_w2v_avg_below25flip=Combined (\\textless25\\%)"


Sentics
-------

compile_results.py \
-e run_0/bias_svm_w2v_avg,run_0/bias_svm_w2v_limit_avg,run_0/bias_svm_w2v_sentic_avg,run_0/bias_svm_w2v_sentic_full_avg,run_0/bias_svm_w2v_sentic_avg_std,run_0/bias_svm_w2v_sentic_full_avg_std \
-o bias_sentic \
--caption "Sentics incorporation." \
--column-replacements "bias_svm_w2v_avg=No sentics,bias_svm_w2v_limit_avg=Limited,bias_svm_w2v_sentic_avg=Sentics,bias_svm_w2v_sentic_full_avg=Padded sentics"


ML Algorithm
------------

compile_results.py \
-e run_0/bias_svm_w2v_avg,seq/bias_w2v_seq,run_0/bias_w2v_avg \
-o bias_alg \
--caption "ML algorithm comparison." \
--column-replacements "bias_svm_w2v_avg=SVM,bias_w2v_avg=NN,bias_w2v_seq=LSTM"
